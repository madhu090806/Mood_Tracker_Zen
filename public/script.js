const video = document.getElementById('video');
const ratingDisplay = document.getElementById('rating');
const emojiDisplay = document.getElementById('emoji');
const progressBar = document.getElementById('progress');

let moodRating = 5;

// âœ… Emotion weights
const emotionWeight = {
  angry: 1,
  disgusted: 2,
  fearful: 2,
  sad: 0,
  neutral: 5,
  surprised: 7,
  happy: 10
};

// âœ… Emoji list for scores 0â€“10
const moodEmojis = ["ðŸ˜­","ðŸ˜¢","â˜¹ï¸","ðŸ˜Ÿ","ðŸ˜","ðŸ™‚","ðŸ˜Š","ðŸ˜ƒ","ðŸ˜„","ðŸ˜","ðŸ¤©"];

console.log("ðŸ“Œ Mood Tracker Starting...");

// âœ… Load models
Promise.all([
  faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
  faceapi.nets.faceExpressionNet.loadFromUri('/models')
])
.then(startVideo)
.catch(err => console.error("âŒ Model Load Error:", err));

function startVideo() {
  navigator.mediaDevices.getUserMedia({ video: true })
    .then(stream => video.srcObject = stream)
    .catch(err => console.error("âŒ Webcam Error:", err));
}

video.addEventListener('play', () => {
  const canvas = faceapi.createCanvasFromMedia(video);
  document.body.append(canvas);
  const displaySize = { width: video.width, height: video.height };
  faceapi.matchDimensions(canvas, displaySize);

  setInterval(async () => {
    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
      .withFaceExpressions();

    if (detections.length > 0) {
      const expressions = detections[0].expressions;
      let score = 0;

      // âœ… Weighted score
      for (const [emotion, prob] of Object.entries(expressions)) {
        score += prob * (emotionWeight[emotion] || 5);
      }

      // âœ… Final mood (0â€“10)
      moodRating = Math.min(10, Math.max(0, Math.round(score)));

      // âœ… Update UI
      ratingDisplay.textContent = moodRating;
      emojiDisplay.textContent = moodEmojis[moodRating];
      progressBar.style.width = `${(moodRating / 10) * 100}%`;
    }
  }, 2000);
});
